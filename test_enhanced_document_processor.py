#!/usr/bin/env python3

"""
Enhanced Document Processor Test
===============================

Tests the updated Document Analysis Pod with context-aware chunking
to validate MarkdownHeaderTextSplitter integration.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import our updated document processor
from document_processor import document_processor

async def test_enhanced_chunking():
    """Test the updated document processor with context-aware chunking."""
    print("ğŸš€ Testing Enhanced Document Processor")
    print("=" * 60)
    
    # Test file
    test_pdf = "car24_chpt1_0.pdf"
    
    if not Path(test_pdf).exists():
        print(f"âŒ Test file not found: {test_pdf}")
        return False
    
    print(f"ğŸ“„ Processing: {test_pdf}")
    
    try:
        # Process the document with our enhanced processor
        result = await document_processor.process_document(test_pdf)
        
        if not result.success:
            print(f"âŒ Processing failed: {result.error}")
            return False
        
        # Analyze results
        chunks = result.documents
        total_chunks = len(chunks)
        
        print(f"âœ… Processing successful!")
        print(f"ğŸ“Š Results:")
        print(f"  - Total chunks: {total_chunks}")
        print(f"  - File type: {result.file_type}")
        
        # Check for context-aware metadata
        header_chunks = 0
        sample_chunks = []
        
        for i, chunk in enumerate(chunks[:5]):  # Check first 5 chunks
            metadata = chunk.metadata
            content = chunk.page_content
            
            # Look for header metadata (our enhancement)
            has_headers = any(key.startswith('Header') for key in metadata.keys())
            if has_headers:
                header_chunks += 1
            
            sample_chunks.append({
                'chunk_id': i + 1,
                'size': len(content),
                'has_headers': has_headers,
                'metadata_keys': list(metadata.keys()),
                'content_preview': content[:100].replace('\n', ' ')
            })
        
        # Count total header chunks
        total_header_chunks = sum(1 for chunk in chunks 
                                if any(key.startswith('Header') for key in chunk.metadata.keys()))
        
        print(f"  - Chunks with header metadata: {total_header_chunks}/{total_chunks}")
        print(f"  - Header detection: {'âœ… Working' if total_header_chunks > 0 else 'âŒ Not detected'}")
        
        # Show sample results
        print(f"\nğŸ“ First 5 Chunks Analysis:")
        for chunk_info in sample_chunks:
            print(f"\n  Chunk {chunk_info['chunk_id']} ({chunk_info['size']} chars):")
            if chunk_info['has_headers']:
                headers = [k for k in chunk_info['metadata_keys'] if k.startswith('Header')]
                print(f"    ğŸ·ï¸  Headers: {headers}")
            print(f"    ğŸ“„ Metadata: {chunk_info['metadata_keys']}")
            print(f"    ğŸ“ Preview: {chunk_info['content_preview']}...")
        
        # Test statistics
        print(f"\nğŸ“ˆ Enhanced Features Test:")
        stats = document_processor.get_processing_stats()
        
        print(f"  - Context-aware chunking: {stats.get('context_aware_chunking', False)}")
        print(f"  - Chunking methods: {stats.get('chunking_methods', [])}")
        print(f"  - Supported headers: {stats.get('supported_headers', [])}")
        print(f"  - Structure detection: {stats.get('structure_detection', False)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        return False

async def compare_with_previous():
    """Compare with our previous chunking results."""
    print(f"\nğŸ” Comparison with Previous Results")
    print("-" * 50)
    
    # Load our previous successful results
    previous_results_file = "car24_chpt1_0_chunking_results.json"
    
    if Path(previous_results_file).exists():
        import json
        
        with open(previous_results_file, 'r') as f:
            previous_data = json.load(f)
        
        prev_chunks = previous_data.get('chunking_results', {}).get('total_chunks', 0)
        prev_method = previous_data.get('chunking_results', {}).get('chunking_method', 'unknown')
        prev_avg_size = previous_data.get('chunking_results', {}).get('avg_chunk_size', 0)
        
        print(f"ğŸ“Š Previous Results:")
        print(f"  - Method: {prev_method}")
        print(f"  - Chunks: {prev_chunks}")
        print(f"  - Avg size: {prev_avg_size:.1f} chars")
        
        # Now test our updated processor
        try:
            result = await document_processor.process_document("car24_chpt1_0.pdf")
            if result.success:
                new_chunks = len(result.documents)
                new_avg_size = sum(len(doc.page_content) for doc in result.documents) / new_chunks
                
                print(f"\nğŸ“Š New Results (Integrated):")
                print(f"  - Method: context_aware (integrated)")
                print(f"  - Chunks: {new_chunks}")
                print(f"  - Avg size: {new_avg_size:.1f} chars")
                
                # Compare
                print(f"\nâš–ï¸  Comparison:")
                print(f"  - Chunk difference: {new_chunks - prev_chunks:+d}")
                print(f"  - Size difference: {new_avg_size - prev_avg_size:+.1f} chars")
                
                if new_chunks > 0:
                    header_count = sum(1 for doc in result.documents 
                                     if any(k.startswith('Header') for k in doc.metadata.keys()))
                    header_percentage = (header_count / new_chunks) * 100
                    print(f"  - Header metadata: {header_count}/{new_chunks} ({header_percentage:.1f}%)")
        except Exception as e:
            print(f"âŒ New processing failed: {e}")
    else:
        print("ğŸ“ No previous results file found")

async def main():
    """Main test function."""
    print("ğŸ§ª Enhanced Document Processor Validation")
    print("=" * 70)
    
    # Test 1: Basic functionality
    success = await test_enhanced_chunking()
    
    if success:
        print(f"\nâœ… TEST PASSED: Enhanced document processor is working!")
        
        # Test 2: Comparison
        await compare_with_previous()
        
        print(f"\nğŸ‰ INTEGRATION SUCCESSFUL!")
        print(f"ğŸ“‹ Summary:")
        print(f"  âœ… MarkdownHeaderTextSplitter integrated")
        print(f"  âœ… Context-aware chunking active")
        print(f"  âœ… Header metadata preservation working")
        print(f"  âœ… Structure detection functional")
        print(f"  âœ… Fallback to recursive splitting available")
        
        print(f"\nğŸš€ Your Document Analysis Pod now has:")
        print(f"  - Intelligent document structure detection")
        print(f"  - Page-level context awareness") 
        print(f"  - Rich header metadata for AI understanding")
        print(f"  - Better semantic coherence in chunks")
    else:
        print(f"\nâŒ TEST FAILED: Issues detected with enhanced processor")
        print(f"ğŸ”§ Check:")
        print(f"  - LangChain dependencies installed")
        print(f"  - MarkdownHeaderTextSplitter import working")
        print(f"  - Virtual environment activated")

if __name__ == "__main__":
    asyncio.run(main())