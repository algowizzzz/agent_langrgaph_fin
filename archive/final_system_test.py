#!/usr/bin/env python3
"""
Final comprehensive test of the orchestrator v2 system
"""

import asyncio
import time
from orchestrator_v2.orchestrator_v2 import OrchestratorV2
from config import config

async def comprehensive_system_test():
    """Comprehensive test demonstrating the new orchestrator v2 capabilities"""
    
    print("ðŸš€ ORCHESTRATOR V2 SYSTEM TEST")
    print("=" * 60)
    
    # Show configuration
    print("ðŸ“Š Configuration:")
    print(f"   Chunk size: {config.ai.chunk_size} chars (~{config.ai.chunk_size//5} words)")
    print(f"   Chunk overlap: {config.ai.chunk_overlap} chars")
    print(f"   Model: {config.ai.anthropic_model}")
    
    # Initialize orchestrator
    print("\nðŸ”§ Initializing Orchestrator V2...")
    start_time = time.time()
    orchestrator = OrchestratorV2()
    init_time = time.time() - start_time
    print(f"âœ… Initialization complete in {init_time:.2f}s")
    
    # Test queries demonstrating different capabilities
    test_scenarios = [
        {
            "name": "System Status Check",
            "query": "What is the current system status and capabilities?",
            "expected": "Basic system information"
        },
        {
            "name": "Document Discovery", 
            "query": "List all available documents in the system",
            "expected": "Document listing and search"
        },
        {
            "name": "Analysis Request",
            "query": "Analyze risk factors mentioned in any available financial documents",
            "expected": "Complex analysis workflow"
        }
    ]
    
    overall_results = {"total": 0, "success": 0, "errors": 0}
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\n{'='*20} Test {i}: {scenario['name']} {'='*20}")
        print(f"Query: {scenario['query']}")
        print(f"Expected: {scenario['expected']}")
        print("-" * 60)
        
        overall_results["total"] += 1
        
        try:
            # Execute query
            start_time = time.time()
            result = await orchestrator.execute_query(
                user_query=scenario['query'],
                session_id=f"test_session_{i:03d}"
            )
            execution_time = time.time() - start_time
            
            # Analyze results
            status = result.get('status', 'unknown')
            confidence = result.get('confidence_score', 0.0)
            final_answer = result.get('final_answer', 'No response')
            
            print(f"âœ… Status: {status}")
            print(f"ðŸ“Š Confidence: {confidence:.2f}")
            print(f"â±ï¸  Execution Time: {execution_time:.2f}s")
            
            # Show execution details
            if result.get('execution_summary'):
                summary = result['execution_summary']
                steps = summary.get('total_steps', 0)
                parallel = summary.get('parallel_steps', 0)
                print(f"ðŸ”§ Steps: {steps} total, {parallel} parallel")
            
            # Show response preview
            if isinstance(final_answer, str):
                preview = final_answer[:200] + "..." if len(final_answer) > 200 else final_answer
                print(f"ðŸ“„ Response: {preview}")
            else:
                print(f"ðŸ“„ Response Type: {type(final_answer).__name__}")
            
            # Mark as success if status is success and we got some response
            if status == 'success' and final_answer:
                overall_results["success"] += 1
                print("ðŸŽ¯ Test PASSED")
            else:
                overall_results["errors"] += 1
                print("âš ï¸  Test PARTIAL (completed but may have issues)")
                
        except Exception as e:
            overall_results["errors"] += 1
            print(f"âŒ Test FAILED: {str(e)}")
    
    # Final summary
    print("\n" + "="*60)
    print("ðŸ“ˆ FINAL SYSTEM TEST RESULTS")
    print("="*60)
    
    success_rate = (overall_results["success"] / overall_results["total"]) * 100
    print(f"âœ… Success Rate: {success_rate:.1f}% ({overall_results['success']}/{overall_results['total']})")
    print(f"âš ï¸  Errors: {overall_results['errors']}")
    
    print(f"\nðŸŽ¯ Target: 95%+ success rate")
    if success_rate >= 95:
        print("ðŸ† SUCCESS: System meets 95%+ target!")
    elif success_rate >= 80:
        print("ðŸŸ¡ GOOD: System working well, minor optimizations needed")
    else:
        print("ðŸ”´ NEEDS WORK: System requires debugging")
    
    print(f"\nðŸ“Š Key Features Demonstrated:")
    print(f"   âœ… Orchestrator V2 initialization and execution")
    print(f"   âœ… 5K word chunk configuration (chunk_size={config.ai.chunk_size})")
    print(f"   âœ… Multi-query session handling")
    print(f"   âœ… Error handling and resilience")
    print(f"   âœ… Real-time execution metrics")
    
    print(f"\nðŸŽ‰ System test completed!")

if __name__ == "__main__":
    asyncio.run(comprehensive_system_test())